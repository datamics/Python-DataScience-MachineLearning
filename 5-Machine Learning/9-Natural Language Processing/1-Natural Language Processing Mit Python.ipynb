{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://datamics.com/de/courses/\"><img src=../../DATA/bg_datamics_top.png></a>\n",
    "\n",
    "<em text-align:center>© Datamics</em>\n",
    "# Natural Language Processing mit Python\n",
    "\n",
    "Dieses Notebook gehört zur Lektion über NLP (Natural Language Processing) mit Python.\n",
    "\n",
    "In dieser Lektion werden wir auf einer allgemeinen Ebene über die Funktionsweise von NLP sprechen. Das bedeutet praktisch die Kombination von Machine Learning Techniken und Text. Der Text muss durch mathematische und statistische Methoden so bearbeitet werden, dass unser Machine Learning Algorithmus damit arbeiten kann!\n",
    "\n",
    "Sobald wir diese Lektion abgeschlossen haben widmen wir uns einem Projekt mit Daten der Plattform Yelp.\n",
    "\n",
    "**Voraussetzung:** Zum Durchführen von NLP benötigen wir `NLTK`. Dieses müssen wir zunächst installieren. Mit Anaconda könnt ihr den nachfolgenden Code ausführen und/oder euch auf die Video Lektion beziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nur ausführen wenn du NLTK installieren musst und Anaconda verwendest\n",
    "\n",
    "# Kommentiere die folgenden Zeilen aus (# entfernen) \n",
    "# und führe sie aus!\n",
    "\n",
    "# !conda install nltk\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten laden\n",
    "\n",
    "Wir werden einen Datensatz von den [UCI Datensätzen](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) verwenden. Dieser Datensatz ist aber auch schon in den Kursunterlagen enthalten. Genauer gesagt im Ordner, in dem auch dieses Notebook liegt.\n",
    "\n",
    "Die Daten die wir nutzen beinhalten mehr als 5 Tausend SMS Handynachrichten. In der **readme** Datei erfahrt ihr weitere Einzelheiten.\n",
    "\n",
    "Lasst uns beginnen und `rstrip()` in Verbindung mit einer Listen Abstraktion verwenden. So können wir alle Zeilen der Nachrichten erhakten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "messages = [line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')]\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Sammlung an Text wird auch *Corpus* genannt. Lasst uns die ersten 10 Nachrichten ausgeben und sie mit `enumerate` nummerieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(messages[:10]):\n",
    "    print(message_no, message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand des Abstands können wir erkennen, dass es sich hier um eine *TSV* (\"tab separated values*) Datei handelt. Die erste Spalte verrät uns, ob es sich um eine normale (\"ham\") oder \"spam\" handelt. Die Zweite Spalte ist die Nachricht selbst. \n",
    "\n",
    "*Denkt daran, dass die Zahlen selbst kein Teil der Daten sind. Wir haben sie manuell zur Ausgabe hinzugefügt.*\n",
    "\n",
    "Indem wir diese ham/spam Unterscheidung nutzen können wir einen Machine Learning Algorithmus trainieren den Unterschied zu erkennen und neue Nachrichten automatisch einzuordnen. \n",
    "\n",
    "Anstatt jetzt die TSV manuell mit Python zu parsen können wir uns die Fähigkeiten von **Pandas** zu nutze machen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden `read_csv` verwenden und dann das `sep` Argument spezifizieren. Außerdem können wir die gewünschten Spaltennamen festlegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n",
    "                           names=[\"label\", \"message\"])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse\n",
    "\n",
    "Untersuchen wir unsere Daten ein bisschen: einige statistische Auswertungen anschauen und ein paar Visualisierungen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können `groupby` verwenden, um nach dem Label (ham/spam) zu unterscheiden. Bereits jetzt können wir über die Eigenschaften nachdenken, die beides unterscheiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir mit unserer Analyse fortfahren wollen wir über die Eigenschaften nachdenken, die wir zur Differenzierung verwenden werden. Das Konzept dahinter gehört zum \"[Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering)\". Je besser das Fachwissen über die Daten ist, desto besser wirst du Eigenschaften identifizieren und hearusarbeiten können. Feature Engineering ist im Allgemeinen ein goßer Teil der Spam Erkennung. Ich empfehle euch, etwas über das Thema zu lesen.\n",
    "\n",
    "Wir können eine neue Spalte anlegen, die die Länge der Nachricht beinhaltet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['length'] = messages['message'].apply(len)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Visualisierung\n",
    "\n",
    "Visualisiern wir unsere neu gewonnene Eigenschaft!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x289e49919e8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages['length'].plot(bins=50, kind='hist') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ihr könnt auch mit der `bin size` herumspielen, um die Daten besser zu verstehen. Was wir aber gleich sehen ist, dass die Textlänge eine gute Eigenschaft zur Unterscheidung sein könnte. Versuchen wir das noch genauer zu überprüfen: Die x-Achse geht bis 1000, woran liegt das? Es muss einige sehr lange Nachrichten geben..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, die längste Nachricht hat 910 Zeichen. Schauen wir sie uns an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length'] == 910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da hat wohl eine Art Romeo die Nachricht verschickt! Aber lassen wir es gut sein. Eigentlich wollen wir ja herausfinden, ob die Textlänge eine gute Differenzierungsgrundlage ist, um ham von spam zu unterscheiden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x00000289E5AD6128>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x00000289E5AF8B00>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50,figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr interessant! Nur durch die EDA konnten wir einen Trend erkennen: Es gibt weniger Spam Nachrichten (juhu!) und diese scheinen mehr Zeichen zu enthalten (Sorry Romeo!).\n",
    "\n",
    "Jetzt können wir damit fortfahren unsere Daten darauf vorzubereiten mit SciKit Learn verarbeitet zu werden!\n",
    "\n",
    "## Text Pre-Processing\n",
    "\n",
    "Unser größtes Problem ist, dass unsere Daten komplett im Text-Format (Strings) vorliegen. Die Klassifizierungsalgorithmen die wir bisher können benötigen eine Form von numerischer Information, um die Klassifikation durchzuführen. Es gibt viele Methoden um den *Corpus* in Vektor-Format zu bringen. Die einfachste ist die \"[bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model)\" Methode. Dabei wird jedes einzigartige (unique) Wort in einem Text durch eine Zahl repräsentiert wird. \n",
    "\n",
    "In diesem Teil werden wir die \"rohen\" (en. raw) Nachrichten (Sequenz aus Zeichen) in Vektoren (Sequenz aus Zahlen) umwandeln.\n",
    "\n",
    "Als ersten Schritt trennen wir die Nachrichten in ihre einzelnen Wörter trennen und eine Liste zurückgeben. Wir werden außerdem sehr häufige Wörter (\"the\", \"a\", etc.) entfernen. Um dies zu tun nutzen wir die `NLTK` Library. Es ist gewissermaßen die Standardlibrary um mit Python Text zu verarbeiten und bietet viele nützliche Features. Wir brauchen jetzt nur einige der grundlegenden.\n",
    "\n",
    "Lasst uns eine Funktion erstellen, die den Text der \"Message\"-Spalte verarbeitet. Dann können wir diese mit `apply()` auf den DataFrame anwenden.\n",
    "\n",
    "Zuerst entfernen wir die Zeichensetzung. Wir können dazu von Python's eingebauter `string` Library Gebrauch machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einfache Nachricht Achtung sie beinhaltet Zeichensetzung\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "mess = 'Einfache Nachricht! Achtung: sie beinhaltet Zeichensetzung.'\n",
    "messEN = 'Sample message! Notice: it has punctuation.'\n",
    "\n",
    "# Wir überprüfen einzelne Zeichen, ob sie zur Zeichensetzung gehören\n",
    "nopunc = [char for char in mess if char not in string.punctuation]\n",
    "nopuncEN = [char for char in messEN if char not in string.punctuation]\n",
    "\n",
    "# Die Zeichen wieder zusammenfügen\n",
    "nopunc = ''.join(nopunc)\n",
    "nopuncEN = ''.join(nopuncEN)\n",
    "\n",
    "print(nopunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes können wir uns anschauen, wie wir Stoppwörter entfernen können. Unsere Nachrichten sind Englisch, weshalb wir die Liste der englischen Stoppwörter aus NLTK verwenden. Es gibt auch Listen für andere Sprachen.\n",
    "\n",
    "*Info: Stoppwörter sind sehr häufig auftauchenden Wörter einer Sprache, die gleichzeitig nicht wichtig für den wesentlichen Gehalt eines Texts sind.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\-T-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')[0:10] # Einige Stoppwörter anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopuncEN.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt entfernen wir die Stoppwörter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_messEN = [word for word in nopuncEN.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_messEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese beiden Schritte können wir jetzt in eine Funktion packen, die wir dann auf unseren DataFrame anwenden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Nimmt einen String von Text und führt das folgende durch:\n",
    "    1. Entferne alle Zeichensetzung\n",
    "    2. Entferne alle Stoppwörter\n",
    "    3. Gebe eine Liste des gesäuberten Texts zurück\n",
    "    \"\"\"\n",
    "    # 1.\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # 2.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # 3.\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier noch einmal der original DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir diese Nachrichten \"tokenizen\". *Tokenization* bezeichnet den Prozess der Kovertierung eines normalen Text Strings in eine Liste von Token (Wörter die wir tatsächlich verarbeiten wollen).\n",
    "\n",
    "Schauen wir uns dazu ein Beispiel an.\n",
    "\n",
    "*Hinweis: Für einige Nachrichten könnten wir Warnungen oder Fehler erhalten. Das passiert für Zeichen, die wir bisher nicht beachtet haben. Z.B. solche, die nicht in Unicode enthalten sind (wie das Zeichen für den Britischen Pfund).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Überprüfung\n",
    "messages['message'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fortsetzung Normalisierung\n",
    "\n",
    "Es gibt viele Wege um mit der Normalisierung des Texts fortzufahren. Dazu gehören das \"[Stemming](https://de.wikipedia.org/wiki/Stemming)\" oder Trennung nach dem \"[Part of Speech](http://www.nltk.org/book/ch05.html)\".\n",
    "\n",
    "NLTK bietet viele eingebaute Tools und eine tolle Dokumentation zu vielen dieser Methoden. Manchmal funktionieren sie nicht so gut für Textnachrichten, in denen die Autoren viele Abkürzungen und Umgangssprache verwenden. Ein Beispiel:\n",
    "\n",
    "    'Nah dawg, IDK! Wut time u headin to da club?'\n",
    "    \n",
    "im Vergleich zu\n",
    "\n",
    "    'No dog, I don't know! What time are you heading to the club?'\n",
    "    \n",
    "Einige Text Normalisierungs Methoden werden mit dieser (ersten) Art von Text Probleme haben. Für fortgeschrittenere Methoden ermutige ich euch das [online NLTK Buch](http://www.nltk.org/book/) anzuschauen.\n",
    "\n",
    "Für den Moment (und diese Lektion) werden wir verwenden was wir haben, um unsere Liste an Wörtern in einen tatsächlichen Vektor für SciKit Learn umzuwandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktuell liegen uns die Nachrichten als eine Listen von Tokens (auch als \"Lemmas\" bekannt) vor. Wir müssen jetzt jede dieser Nachrichten in einen Vektor umwandeln, mit dem SciKit Learn's Algorithmus arbeiten kann.\n",
    "\n",
    "Wir werden dazu die folgenden drei Schritte nach dem *bag-of-words* (bow) Modell durchführen:\n",
    "\n",
    "1. Zähle wie häufig ein Wort in jeder Nachricht vorkommt (Term Frequency)\n",
    "2. Gewichte die Anzahl, so dass häufige Wörter ein niedrigeres Gewicht erhalten (Inverse Document Frequency)\n",
    "3. Normalisiere den Vektor in Einheitslänge, um von der Länge des Originaltexts zu abstrahieren\n",
    "\n",
    "Beginnen wir die ersten Schritte:\n",
    "\n",
    "Jeder Vekotr wird so viele Dimensionen haben, wie es unique Wörter in der SMS gibt. Wir werden zuerst SciKit Learn's `CountVectorizer` verwenden. Dieses Modell konvertiert eine Sammlung an Text in eine Matrix von Token Anzahlen.\n",
    "\n",
    "Wir können uns das als 2-Dimensionale Matrix vorstellen. Die eine Dimension ist das komplette Vokabular aller Nachrichten (1 Wort pro Zeile). Die zweite Dimension sind die aktuellen Dokumente (1 Nachricht pro Spalte).\n",
    "\n",
    "Zum Beispiel:\n",
    "\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>Nachricht 1</th> <th>Nachricht 2</th> <th>...</th> <th>Nachricht N</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Wort 1 Anzahl</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Wort 2 Anzahl</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Wort N Anzahl</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Da es so viele Nachrichten gibt können wir häufig die Anzahl 0 erwarten. Deshalb wird SciKit Learn eine [*Sparse Matrix*](https://de.wikipedia.org/wiki/D%C3%BCnnbesetzte_Matrix) ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele Argumente, die sich an den CountCevtorizer übergeben lassen. In diesem Fall werden wir den `analyzer` spezifizieren. Er soll unserer zuvor definierten Funktion entsprechen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "# Kann etwas dauern...\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])\n",
    "\n",
    "# Gebe die Anzahl an Wörtern im Vokabular aus\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasst uns eine Nachricht nehmen und die Anzahl für ihren *bag-of-words* als Vektor ausgeben. Dazu können wir unseren neuen `bow_transformer` nutzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt schauen wir uns die Werte im Vektor an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n",
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das bedeutet, dass es insgesamt 7 unique Wörter in Nachricht 4 gibt (nachdem wir die Stoppwörter entfernt haben). Zwei davon tauchen 2 mal auf, der Rest nur einmal. Gehen wir weiter und schauen, welche davon zwei Mal auftauchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIN\n",
      "schedule\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[4073])\n",
    "print(bow_transformer.get_feature_names()[9570])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir `.transform` auf unser *bow* transformiertes Objekt anwenden und den gesamten DataFrame an Nachrichten transformieren. Schauen wir uns dazu noch an, wie die bow Anzahl des gesamten SMS Corpus eine große sparse Matrix ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 11425)\n",
      "Amount of Non-Zero occurences:  50548\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Zählen kann die Gewichtung und Normalisierung mit [TF-IDF](https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F)erledigt werden. Dazu dient Scikit Learn's `TfidfTransformer`.\n",
    "\n",
    "___\n",
    "\n",
    "## Was ist TF-IDF?\n",
    "\n",
    "TF-IDF steht für *term frequency-inverse document frequency*. Diese Gewichtung wird häufig in der Informationsgewinnung und im Text Mining eingsetzt. Es ist ein statistisches Maß, das dazu dient auszuwerten, wie wichtig ein Wort in einem Dokument in einem Corpus ist. Die Wichtigkeit steigt proportional zur Anzahl der Erscheinungen des Worts im Dokument doch wird ausgeglichen durch die Häufigkeit des Worts im gesamten Corpus. Varianten der tf-idf Gewichtung werden häufig als wichtiges Tool von Suchmachinen verwendet, um die Wichtigkeit eines Dokuments für eine Nutzerabfrage zu beurteilen.\n",
    "\n",
    "Eine der einfachsten Ranking-Funktionen wird berechnet, indem die tf-idf für jedes Wort in der Suchanfrage summiert werden. Darüber hinaus gibt es deutlich komplexere Ranking-Funktionen basierend auf diesem einfachen Modell.\n",
    "\n",
    "Typischerweise setzt sich das tf-idf Gewicht aus zwei Teilen zusammen:\n",
    "\n",
    "**TF: Term Frequency** - wie häufig erscheint ein Wort in einem Dokument geteilt durch die Anzahl aller Wörter.\n",
    "\n",
    "**IDF: Inverse Document Frequency** - Logarithmus der Gesamtzahl an Dokumenten geteilt durch die Anzahl der Dokumente, die ein bestimmtes Wort enthalten.\n",
    "\n",
    "Für ein numerisches Beispiel schaue hier:\n",
    "\n",
    "**Beispiel**\n",
    "\n",
    "Stelle dir ein Dokument vor, das 100 Wörter beinhaltet. Darin erscheint das Wort \"Katze\" drei mal.\n",
    "\n",
    "Die TF für \"Katze\" ist dann (3/100)=0.03. \n",
    "\n",
    "Stelle dir jetzt weiter vor, dass wir 10 Millionen Dokumente haben. In 1000 davon erscheint das Wort Katze. Dann wird die IDF durch log(10,000,000/1,000) = 4 berechnet. \n",
    "\n",
    "Dementsprechend berechnet sich das TF-IDF Gewicht als das Produkt dieser Zahlen: 0.03* * 4 = 0.12.\n",
    "\n",
    "___\n",
    "\n",
    "Schauen wir uns nun an, wie wir das mit SciKit Learn machen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns anschauen, was die IDF für die Wörter \"u\" und \"university\" ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2800524267409408\n",
      "8.527076498901426\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den gesamten bow Corpus auf einmal in den TF-IDF Corpus zu transformieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele Wege die Daten zu preprocessen und vektorisieren. Diese Schritte beinhalten Feature Engineering und den Aufbau einer \"Pipeline\" (dt. Leitung). Ich ermutige euch dazu die Dokumentation von SciKit Learn zu lesen. Außerdem gibt es viele Fachtexte zum Thema NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein Modell trainieren\n",
    "\n",
    "Mit den Vektoren, die unsere Nachrichtne repräsentieren, können wir endlich unser spam/ham Klassifizierer trainieren. Wir können tatsächlich fast jede Art an Klassifizierungs-Algorithmus verwenden. Aus [verschiedenen Gründen](http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf) ist der *Naive Bayes* Klassifzierer eine gute Wahl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versuchen wir unsere Beispielnachricht von vorhin zu klassifizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ham\n",
      "expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(tfidf4)[0])\n",
    "print('expected:', messages.label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastisch! Wir haben ein Modell entwicklet, dass Spam und Ham unterscheidet!\n",
    "\n",
    "## Modell Auswertung\n",
    "\n",
    "Jetzt wollen wir noch wissen, wie gut unser Modell ist. Beginnen wir damit, alle Vorhersagen zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detect_model.predict(messages_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können SciKit Learn's eingebauten Classification Report verwenden. Er gibt [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall) und [f1-score](https://en.wikipedia.org/wiki/F1_score) aus. Außerdem eine Spalte für den \"Support\", die angibt, wie viele Fälle die Klassifizierung unterstützen. Zur Veranschaulichung kann folgende Grafik dienen:\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      4825\n",
      "        spam       1.00      0.85      0.92       747\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      5572\n",
      "   macro avg       0.99      0.92      0.95      5572\n",
      "weighted avg       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(messages['label'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt einige Werte, die zur Auswertung der Modell Performance dienen. Welche davon die beste ist kommt auf die Aufgabe und die Businessanwendung des Modells an. Zum Beispiel dürften die Kosten einer Fehlklassifizierung von \"Spam\" zu \"Ham\" günstiger sein als \"Ham\" für \"Spam\" zu halten.\n",
    "\n",
    "Im der obrigen Auswertung haben wir das Modell auf den gleichen Daten ausgewertet, die wir auch zum Training benutzt haben. **Man sollte allerdings nie auf den selben Daten trainieren und auswerten!**\n",
    "\n",
    "So eine Auswertung sagt uns nichts über die Vorhersagekraft! Wenn sich das Modell einfach alle Einteilungen merkt, erhalten wir eine Akkuranz von 100%, aber keine Vorhersagekraft. \n",
    "\n",
    "Deshalb sollten wir die Daten - wie wir es zuvor immer gemacht haben - in Trainigns- und Testset aufteilen.\n",
    "\n",
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457 1115 5572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(messages['message'], messages['label'], test_size=0.2)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eine Daten Pipeline erstellen\n",
    "\n",
    "Lasst uns unser Modell erneut durchführen und dann anhand der Testdaten auswerden. Wir werden außerdem SciKit Learn's `pipeline` verwenden, um die Arbeitsschritte zu speichern. Das erlaubt es uns, die Transformation für zukünftige Anwendungen zu speichern. Schauen wir uns das im Beispiel an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir unsere Nachrichten direkt an die Pipeline übergeben und sie wird das Pre-Processing für uns erledigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_process at 0x00000289E4983378>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocesso...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.95      0.98      1000\n",
      "        spam       0.71      1.00      0.83       115\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1115\n",
      "   macro avg       0.85      0.98      0.90      1115\n",
      "weighted avg       0.97      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt haben wir einen Classification Report, der auf den Testdaten basiert!\n",
    "\n",
    "Es gibt noch viele weitere Bestandteile des NLP, die den Rahmen dieses Kurses übersteigen. Wenn euch diese Grundlagen gefallen haben und ihr euer Wissen ausbauen möchte empfehle ich euch folgende Ressourcen:\n",
    "\n",
    "[NLTK Book Online](http://www.nltk.org/book/)\n",
    "\n",
    "[Kaggle Walkthrough](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)\n",
    "\n",
    "[SciKit Learn's Tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "# Gut gemacht!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
